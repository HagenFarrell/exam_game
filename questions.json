[
    {
        "topic": "Review: CNNs",
        "text": "<b>[Must Know]</b> Why are CNNs considered more parameter efficient than Fully Connected (FC) layers?",
        "options": [
            "Weight Sharing: A filter is reused across the entire image",
            "Dropout: They drop connections randomly",
            "Pooling: They reduce the dimensionality early",
            "They use fewer layers than FC networks"
        ],
        "correct": 0,
        "rationale": "In an FC layer, every input connects to every neuron. In a CNN, a single kernel (e.g., 5x5) is shared across the whole spatial input, drastically reducing parameters [cite: 9-10]."
    },
    {
        "topic": "Review: CNN Math",
        "text": "<b>[Must Know]</b> What is the correct formula for the output spatial dimension of a Conv Layer?",
        "options": [
            "$$ \\lfloor \\frac{H_{in} - K + 2P}{S} \\rfloor + 1 $$",
            "$$ (H_{in} - K) \\times S + P $$",
            "$$ \\frac{H_{in} + K}{S} - 2P $$",
            "$$ H_{in} - K + 2P + S $$"
        ],
        "correct": 0,
        "rationale": "You take input height, subtract kernel size, add padding (both sides), divide by stride, floor it, and add 1[cite: 12]."
    },
    {
        "topic": "Review: Transformers",
        "text": "<b>[Must Know]</b> Why is <b>Layer Norm</b> used in NLP Transformers instead of Batch Norm?",
        "options": [
            "Sentence lengths vary, making batch statistics unreliable",
            "It is computationally cheaper",
            "Batch Norm causes exploding gradients in RNNs",
            "Layer Norm adds positional information"
        ],
        "correct": 0,
        "rationale": "In NLP, batch statistics are noisy due to varying sequence lengths and small batch sizes. Layer Norm normalizes across the feature dimension for a single sample, independent of the batch [cite: 33-34]."
    },
    {
        "topic": "Review: Transformers",
        "text": "<b>[Must Know]</b> What is the primary purpose of <b>Residual Connections</b> ($$ x + f(x) $$)?",
        "options": [
            "To solve vanishing gradients by providing a signal 'highway'",
            "To increase the number of parameters",
            "To combine the Encoder and Decoder",
            "To normalize the input"
        ],
        "correct": 0,
        "rationale": "Residuals allow gradients to flow through the network without passing through non-linearities, mitigating the vanishing gradient problem in deep networks[cite: 35]."
    },
    {
        "topic": "Review: SSL",
        "text": "<b>[Must Know]</b> What is the 'Collapse Problem' in Self-Supervised Learning?",
        "options": [
            "The model outputs a constant vector for all inputs to minimize distance",
            "The model stops training early",
            "The gradients become zero immediately",
            "The model memorizes the training data"
        ],
        "correct": 0,
        "rationale": "Without negative pairs (in contrastive learning), the model can 'cheat' by outputting the exact same representation for every image, making the distance zero[cite: 58]."
    },
    {
        "topic": "Review: SSL",
        "text": "<b>[Must Know]</b> How does SimCLR define a 'Positive Pair'?",
        "options": [
            "Two augmentations (crops/colors) of the same image",
            "Two images from the same class",
            "An image and its label",
            "An image and a random image from the batch"
        ],
        "correct": 0,
        "rationale": "SimCLR treats two different views (augmentations) of the same source image as a positive pair that should be pulled together[cite: 55]."
    },
    {
        "topic": "Review: Scaling",
        "text": "<b>[Must Know]</b> What did the <b>Chinchilla</b> scaling laws demonstrate about compute optimality?",
        "options": [
            "Model size (N) and Data size (D) should be scaled equally",
            "Model size is much more important than Data size",
            "Data size is the only thing that matters",
            "Training longer is always better than a larger model"
        ],
        "correct": 0,
        "rationale": "Chinchilla proved that for optimal compute usage, you must scale Parameters (N) and Data (D) in roughly equal proportions[cite: 79]."
    },
    {
        "topic": "Review: Tuning",
        "text": "<b>[Must Know]</b> What is a <b>Soft Prompt</b> in Prompt Tuning?",
        "options": [
            "Learnable continuous vectors prepended to the input",
            "Human-written text instructions",
            "Discrete tokens selected by the model",
            "Random noise added to the input"
        ],
        "correct": 0,
        "rationale": "Soft prompts are learnable vectors (virtual tokens) that are optimized via backprop, unlike hard prompts which are fixed discrete words[cite: 75]."
    },
    {
        "topic": "Review: Alignment",
        "text": "<b>[Must Know]</b> What is the key difference between <b>DPO</b> and <b>RLHF</b>?",
        "options": [
            "DPO removes the separate Reward Model training step",
            "DPO uses a more complex Reward Model",
            "DPO requires more GPU memory",
            "DPO is less stable than PPO"
        ],
        "correct": 0,
        "rationale": "DPO mathematically derives the optimal policy directly from preference data, eliminating the need to train a proxy Reward Model and use PPO[cite: 97]."
    },
    {
        "topic": "Review: Alignment",
        "text": "<b>[Must Know]</b> In DPO, what does the loss function try to do?",
        "options": [
            "Increase likelihood of winning responses relative to the reference model",
            "Minimize the MSE between the reward and the output",
            "Maximize the entropy of the policy",
            "Clone the behavior of the SFT model"
        ],
        "correct": 0,
        "rationale": "DPO increases the probability of the 'winning' response and decreases the 'losing' response, weighted by an implicit reward estimate[cite: 100]."
    },
    {
        "topic": "Review: RL",
        "text": "<b>[Must Know]</b> Which RL algorithm class is considered the <b>least</b> sample efficient?",
        "options": [
            "Evolutionary / Random Guessing",
            "On-Policy (PPO)",
            "Off-Policy (Q-Learning)",
            "Model-Based"
        ],
        "correct": 0,
        "rationale": "Evolutionary strategies do not use gradients or past data effectively, making them the lowest on the efficiency hierarchy[cite: 111]."
    },
    {
        "topic": "Review: RL",
        "text": "<b>[Must Know]</b> In Policy Gradient, why do we subtract a <b>baseline (b)</b>?",
        "options": [
            "To reduce variance without introducing bias",
            "To ensure the gradients are always positive",
            "To increase the learning rate",
            "To calculate the Value function"
        ],
        "correct": 0,
        "rationale": "Subtracting a baseline (like average reward) centers the returns, reducing the variance of the gradient estimator and speeding up convergence[cite: 115]."
    },
    {
        "topic": "Review: CNNs",
        "text": "<b>[Must Know]</b> What does a <b>Stride</b> of 2 do to the output dimensions?",
        "options": [
            "It effectively downsamples the spatial resolution by half",
            "It doubles the spatial resolution",
            "It keeps resolution same but doubles channels",
            "It has no effect on resolution"
        ],
        "correct": 0,
        "rationale": "A higher stride means the filter skips pixels, reducing the output size. Stride 2 roughly halves the height and width[cite: 14]."
    },
    {
        "topic": "Review: CNNs",
        "text": "<b>[Must Know]</b> What is <b>Translation Invariance</b> in the context of CNNs?",
        "options": [
            "A feature (like an eye) is detected regardless of where it is in the image",
            "The model output is invariant to rotation",
            "The model works on different image sizes",
            "The weights are invariant to initialization"
        ],
        "correct": 0,
        "rationale": "Weight sharing assumes translation invariance: a visual feature is useful to detect regardless of its x,y position in the image[cite: 17]."
    },
    {
        "topic": "Review: Scaling",
        "text": "<b>[Must Know]</b> According to Chinchilla, most LLMs before 2022 were...",
        "options": [
            "Severely undertrained (too big, not enough data)",
            "Too small for the data available",
            "Optimally scaled",
            "Overfitted to the training set"
        ],
        "correct": 0,
        "rationale": "Chinchilla showed we used to train huge models on too little data. We should have used smaller models on much more data for compute optimality[cite: 78]."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "Which architecture popularized the use of <b>ReLU</b> and <b>Dropout</b>?",
        "options": [
            "AlexNet (2012)",
            "LeNet-5 (1998)",
            "ResNet (2015)",
            "VGG-16"
        ],
        "correct": 0,
        "rationale": "AlexNet was the breakthrough deep learning model that introduced ReLU and Dropout to prevent overfitting in deep networks[cite: 169]."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "What is the effect of a <b>1x1 Convolution</b>?",
        "options": [
            "It changes the number of channels (depth) without changing spatial size",
            "It blurs the image",
            "It performs downsampling",
            "It is a no-op (does nothing)"
        ],
        "correct": 0,
        "rationale": "1x1 convolutions compute a dot product across the depth dimension, effectively acting as a per-pixel fully connected layer to change channel count[cite: 928]."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "If you have a 32x32x3 image and apply ten 5x5 filters with stride 1 and pad 2, what is the output volume?",
        "options": [
            "32x32x10",
            "28x28x10",
            "32x32x3",
            "30x30x10"
        ],
        "correct": 0,
        "rationale": "Formula: (32 - 5 + 2*2)/1 + 1 = 32. The spatial size is preserved, and depth becomes the number of filters (10)[cite: 868]."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "What is the number of parameters in a CONV layer with ten 5x5x3 filters?",
        "options": [
            "760",
            "750",
            "75",
            "3072"
        ],
        "correct": 0,
        "rationale": "Each filter is 5*5*3 = 75 weights + 1 bias = 76. With 10 filters, 76 * 10 = 760 parameters[cite: 880]."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "What is the main function of a <b>Pooling Layer</b>?",
        "options": [
            "To downsample spatial dimensions and gain invariance",
            "To increase the depth of the network",
            "To add non-linearity",
            "To prevent vanishing gradients"
        ],
        "correct": 0,
        "rationale": "Pooling reduces the size of the representation (downsampling) and makes the model robust to small spatial shifts[cite: 1129]."
    },
    {
        "topic": "Lec 5: Attention",
        "text": "In Self-Attention, how are attention scores calculated?",
        "options": [
            "Dot product of Query (Q) and Key (K)",
            "Dot product of Value (V) and Key (K)",
            "Addition of Query (Q) and Value (V)",
            "Convolution of Q and K"
        ],
        "correct": 0,
        "rationale": "Attention scores are derived by taking the dot product of the Query vector with the Key vectors of the sequence[cite: 1238]."
    },
    {
        "topic": "Lec 5: Architecture",
        "text": "Which model uses a <b>Decoder-Only</b> architecture?",
        "options": [
            "GPT",
            "BERT",
            "T5",
            "BART"
        ],
        "correct": 0,
        "rationale": "GPT models are autoregressive (predict next token) and use only the decoder stack. BERT is encoder-only; T5 is encoder-decoder[cite: 1343]."
    },
    {
        "topic": "Lec 5: Embeddings",
        "text": "What is <b>RoPE</b> (Rotary Positional Encoding)?",
        "options": [
            "A relative encoding that rotates query/key vectors",
            "An absolute sinusoidal encoding added to inputs",
            "A learned embedding layer",
            "A method to randomize token positions"
        ],
        "correct": 0,
        "rationale": "RoPE encodes relative position by rotating the Q and K vectors in the embedding space using trigonometry[cite: 1556]."
    },
    {
        "topic": "Lec 5: Attention",
        "text": "Why do we scale the dot product by $$ 1/\\sqrt{d_k} $$ in Scaled Dot-Product Attention?",
        "options": [
            "To prevent gradients from vanishing in the Softmax",
            "To reduce the memory usage",
            "To make the calculation integer-only",
            "To normalize the batch statistics"
        ],
        "correct": 0,
        "rationale": "As dimension d_k grows, dot products become large, pushing Softmax into regions with extremely small gradients. Scaling prevents this[cite: 1303]."
    },
    {
        "topic": "Lec 5: Norm",
        "text": "What is the difference between Pre-LN and Post-LN?",
        "options": [
            "Pre-LN applies normalization before the sub-layer (more stable)",
            "Post-LN applies normalization before the sub-layer",
            "Pre-LN is used in BERT, Post-LN in GPT",
            "They are mathematically identical"
        ],
        "correct": 0,
        "rationale": "Pre-LN puts the layer norm inside the residual block before the attention/FFN. It is generally more stable for gradient propagation in deep models[cite: 1655]."
    },
    {
        "topic": "Lec 9: Tuning",
        "text": "What did the <b>LIMA</b> paper suggest about alignment?",
        "options": [
            "Less Is More: A few high-quality examples are enough",
            "We need massive RLHF datasets",
            "Pre-training is less important than Fine-tuning",
            "RLHF is strictly required for chat behaviors"
        ],
        "correct": 0,
        "rationale": "LIMA showed that the 'Superficial Alignment Hypothesis' holds: you only need ~1000 high-quality examples to teach the model the style/format[cite: 3146]."
    },
    {
        "topic": "Lec 9: RLHF",
        "text": "In the standard RLHF pipeline (InstructGPT), what are the three steps?",
        "options": [
            "SFT -> Reward Modeling -> PPO",
            "Pretraining -> SFT -> Testing",
            "SFT -> DPO -> Evaluation",
            "Reward Modeling -> SFT -> PPO"
        ],
        "correct": 0,
        "rationale": "The standard pipeline is: 1. Supervised Fine-Tuning (SFT), 2. Train Reward Model (RM), 3. Optimize Policy via PPO[cite: 3248]."
    },
    {
        "topic": "Lec 9: RLHF",
        "text": "What is the purpose of the <b>KL Penalty</b> in RLHF?",
        "options": [
            "To prevent the model from drifting too far from the SFT model",
            "To force the model to be more creative",
            "To increase the reward score",
            "To reduce the training time"
        ],
        "correct": 0,
        "rationale": "The KL term penalizes the RL policy if it diverges too much from the reference (SFT) model, preventing 'reward hacking' (generating gibberish that scores high)[cite: 3419]."
    },
    {
        "topic": "Lec 9: DPO",
        "text": "How does DPO avoid using a separate Reward Model?",
        "options": [
            "It solves for the optimal policy mathematically using preference pairs",
            "It uses GPT-4 as a judge instead",
            "It relies on pure Supervised Learning",
            "It uses a heuristic rule-based reward"
        ],
        "correct": 0,
        "rationale": "DPO uses a closed-form solution to the RLHF objective, allowing optimization directly on the preference data without an explicit RM network[cite: 3672]."
    },
    {
        "topic": "Lec 9: Data",
        "text": "What is <b>Rejection Sampling</b> (Best-of-N) in finetuning?",
        "options": [
            "Sampling N outputs, using RM to pick the best, and training on that",
            "Rejecting data that is too short",
            "Randomly dropping weights",
            "Using human labelers to write new answers"
        ],
        "correct": 0,
        "rationale": "Rejection sampling involves generating multiple candidates from the policy, scoring them with the reward model, and treating the winner as a new ground-truth label[cite: 2044]."
    },
    {
        "topic": "Lec 9: Models",
        "text": "Which model popularized <b>Instruction Tuning</b> (finetuning on many tasks)?",
        "options": [
            "FLAN (Finetuned Language Net)",
            "BERT",
            "GPT-2",
            "Word2Vec"
        ],
        "correct": 0,
        "rationale": "FLAN demonstrated that finetuning a model on a massive mixture of tasks (converted to instructions) significantly boosts zero-shot performance[cite: 2831]."
    },
    {
        "topic": "Lec 3: Activation",
        "text": "What does the <b>ReLU</b> activation function do?",
        "options": [
            "max(0, x)",
            "1 / (1 + e^-x)",
            "tanh(x)",
            "x if x > 0 else 0.1x"
        ],
        "correct": 0,
        "rationale": "ReLU (Rectified Linear Unit) is simply max(0, x). It introduces non-linearity while being computationally efficient and avoiding vanishing gradients for positive inputs[cite: 1694]."
    },
    {
        "topic": "Lec 5: Transformers",
        "text": "What is <b>Multi-Head Attention</b>?",
        "options": [
            "Running attention in parallel multiple times with different projections",
            "Stacking attention layers sequentially",
            "Using multiple models to vote on the answer",
            "Attending to multiple documents at once"
        ],
        "correct": 0,
        "rationale": "Multi-head attention projects Q, K, V into multiple subspaces and runs attention in parallel, allowing the model to focus on different aspects (syntax vs semantics) simultaneously[cite: 1420]."
    },
    {
        "topic": "Lec 5: Architecture",
        "text": "What is the complexity of Self-Attention with respect to sequence length N?",
        "options": [
            "$$ O(N^2) $$",
            "$$ O(N) $$",
            "$$ O(N \\log N) $$",
            "$$ O(1) $$"
        ],
        "correct": 0,
        "rationale": "Self-attention computes a score for every pair of tokens, resulting in an N by N matrix, hence quadratic complexity[cite: 1248]."
    },
    {
        "topic": "Lec 9: RLHF",
        "text": "What is <b>Reward Hacking</b>?",
        "options": [
            "The agent finding a loophole to maximize reward without actual improvement",
            "The reward model overfitting to the training data",
            "The policy gradient vanishing to zero",
            "Using a pre-trained reward model"
        ],
        "correct": 0,
        "rationale": "Reward hacking occurs when the RL agent exploits flaws in the reward function (or model) to get high scores despite producing low-quality or nonsensical outputs[cite: 3593]."
    },
    {
        "topic": "Lec 3: Pooling",
        "text": "What happens during <b>Max Pooling</b> with a 2x2 filter and stride 2?",
        "options": [
            "The spatial resolution is halved",
            "The spatial resolution is doubled",
            "The depth is halved",
            "The image is inverted"
        ],
        "correct": 0,
        "rationale": "A 2x2 max pool with stride 2 takes the maximum value in 2x2 blocks, effectively reducing the height and width by a factor of 2[cite: 1145]."
    },
    {
        "topic": "Lec 5: Embeddings",
        "text": "Why are <b>Positional Encodings</b> necessary in Transformers?",
        "options": [
            "Because self-attention is permutation invariant (bag of words)",
            "To increase the parameter count",
            "To handle large vocabularies",
            "They replace the bias term"
        ],
        "correct": 0,
        "rationale": "Without positional encodings, a Transformer sees 'dog bit man' and 'man bit dog' as identical sets of tokens. Encodings inject order information[cite: 1497]."
    },
    {
        "topic": "Lec 9: Data",
        "text": "What is <b>Self-Instruct</b>?",
        "options": [
            "Using a strong model to generate instruction-response pairs for training",
            "Manually writing instructions",
            "Using the test set for training",
            "Letting the model define its own loss"
        ],
        "correct": 0,
        "rationale": "Self-Instruct involves bootstrapping a model by having a stronger model (like GPT-4) generate synthetic instructions and responses to train a smaller model[cite: 3144]."
    },
    {
        "topic": "Lec 5: Training",
        "text": "What is <b>Warmup</b> in the context of Learning Rate schedules?",
        "options": [
            "Linearly increasing LR from 0 at the start of training",
            "Heating up the GPU before training",
            "Running the model on easy data first",
            "Initializing weights to large values"
        ],
        "correct": 0,
        "rationale": "Warmup involves starting with a low learning rate and increasing it linearly. This stabilizes early training when gradients can be very large[cite: 1736]."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "In LeNet-5, what was the primary application?",
        "options": [
            "Digit Recognition (MNIST)",
            "Face Detection",
            "Object Detection (ImageNet)",
            "Video Analysis"
        ],
        "correct": 0,
        "rationale": "LeNet-5 (1998) was designed by LeCun et al. for gradient-based learning applied to document recognition, specifically digits[cite: 121]."
    },
    {
        "topic": "Lec 5: Components",
        "text": "What is the <b>Feed Forward Network (FFN)</b> in a Transformer block?",
        "options": [
            "Two linear layers with a non-linearity (ReLU/GELU) in between",
            "A convolutional layer",
            "A recurrent layer",
            "A simple matrix multiplication"
        ],
        "correct": 0,
        "rationale": "After attention, each token is processed independently by a position-wise FFN, usually consisting of an expansion layer, activation, and projection layer[cite: 1674]."
    },
    {
        "topic": "Lec 9: Eval",
        "text": "What is <b>MMLU</b>?",
        "options": [
            "Massive Multitask Language Understanding benchmark",
            "A new loss function",
            "A training dataset of code",
            "A reinforcement learning algorithm"
        ],
        "correct": 0,
        "rationale": "MMLU is a comprehensive benchmark covering 57 subjects (STEM, humanities, etc.) used to evaluate LLM knowledge and problem solving[cite: 2961]."
    },
    {
        "topic": "Lec 5: Efficiency",
        "text": "What is <b>Sliding Window Attention</b> (used in Mistral)?",
        "options": [
            "Attention is limited to a local window of neighboring tokens",
            "Attention is calculated over the whole sequence",
            "Attention weights are randomly dropped",
            "Tokens attend only to the first token"
        ],
        "correct": 0,
        "rationale": "To handle long contexts efficiently, Mistral limits attention to a fixed window size (e.g., 4096), reducing complexity from quadratic to linear[cite: 2550]."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "What defines a <b>1x1 Convolution</b> operation?",
        "options": [
            "It computes a weighted sum of channels at a single pixel location",
            "It averages the neighboring pixels",
            "It copies the input to the output",
            "It rotates the image by 1 degree"
        ],
        "correct": 0,
        "rationale": "A 1x1 conv kernel sees only one pixel but all input channels. It is essentially a dense layer applied to every pixel location independently[cite: 929]."
    },
    {
        "topic": "Lec 9: Models",
        "text": "What is the main advantage of <b>Pythia</b> models?",
        "options": [
            "They released intermediate checkpoints to study training dynamics",
            "They are the largest models available",
            "They use a mixture of experts architecture",
            "They are trained only on code"
        ],
        "correct": 0,
        "rationale": "Pythia was designed for science: they released 154 checkpoints throughout training to allow researchers to study how models learn over time[cite: 2365]."
    },
    {
        "topic": "Lec 5: Scaling",
        "text": "What does <b>FP16</b> or <b>BF16</b> refer to?",
        "options": [
            "Low-precision floating point formats (16-bit) to save memory",
            "A type of model architecture",
            "The batch size of 16",
            "The number of attention heads"
        ],
        "correct": 0,
        "rationale": "Training in 32-bit (FP32) is expensive. FP16 and BF16 (Brain Float) use 16 bits, halving memory usage and speeding up matrix math[cite: 1750]."
    },
    {
        "topic": "Lec 9: DPO",
        "text": "In DPO, what serves as the 'Label' for the loss?",
        "options": [
            "The preference pair ($$ y_w, y_l $$)",
            "A scalar score from 1-10",
            "The next token in the sequence",
            "The BLEU score"
        ],
        "correct": 0,
        "rationale": "DPO requires a dataset of pairs, where one response is marked 'winner' and the other 'loser'. It optimizes the likelihood ratio between them[cite: 3676]."
    },
    {
        "topic": "Lec 3: Visualization",
        "text": "What do early layers in a CNN typically detect?",
        "options": [
            "Simple edges and colors",
            "Complete objects like cars",
            "Text and numbers",
            "Abstract concepts"
        ],
        "correct": 0,
        "rationale": "Due to small receptive fields, early layers detect low-level features like edges/gradients. Deeper layers combine these into shapes and objects[cite: 607]."
    },
    {
        "topic": "Lec 5: Norm",
        "text": "What is <b>RMSNorm</b>?",
        "options": [
            "A simplified LayerNorm without mean centering or bias",
            "A normalization using Root Mean Square Error",
            "A batch normalization variant",
            "A norm applied to weights only"
        ],
        "correct": 0,
        "rationale": "RMSNorm (Root Mean Square Norm) simplifies LayerNorm by removing the mean calculation and bias, gaining speed while maintaining performance[cite: 1632]."
    },
    {
        "topic": "Lec 9: RLHF",
        "text": "What is the <b>Bradley-Terry</b> model used for?",
        "options": [
            "Modeling the probability that one item is preferred over another",
            "Generating text",
            "Calculating gradients",
            "Tokenizing input text"
        ],
        "correct": 0,
        "rationale": "The Bradley-Terry model provides a statistical framework to estimate a score function (Reward Model) from pairwise comparison data[cite: 3378]."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "Why do we typically increase the number of filters in deeper layers?",
        "options": [
            "To capture richer, higher-level feature combinations",
            "Because the image size increases",
            "To reduce the computational cost",
            "It is required by the math"
        ],
        "correct": 0,
        "rationale": "As spatial resolution drops (due to pooling), we increase the channel depth (filters) to preserve information capacity and capture complex semantic features[cite: 841]."
    },
    {
        "topic": "Lec 5: Tokenization",
        "text": "What is <b>BPE</b> (Byte Pair Encoding)?",
        "options": [
            "An algorithm to iteratively merge frequent character pairs into tokens",
            "A method to encrypt text",
            "A way to represent images as text",
            "A loss function for training"
        ],
        "correct": 0,
        "rationale": "BPE starts with characters and iteratively merges the most frequent adjacent pairs to form a vocabulary of subword units[cite: 2127]."
    },
    {
        "topic": "Lec 9: Alignment",
        "text": "What is the 'Alignment Tax'?",
        "options": [
            "Performance regression on downstream tasks due to RLHF",
            "The cost of paying human labelers",
            "The time it takes to align a model",
            "A fee paid to OpenAI"
        ],
        "correct": 0,
        "rationale": "Aligning a model (e.g., to be safe/helpful) can sometimes degrade its pure capability on objective benchmarks, a phenomenon known as the alignment tax."
    },
    {
        "topic": "Lec 5: Components",
        "text": "What corresponds to the 'Key' and 'Value' in standard Encoder-Decoder attention?",
        "options": [
            "The output of the Encoder",
            "The previous tokens of the Decoder",
            "The positional embeddings",
            "The random initialization"
        ],
        "correct": 0,
        "rationale": "In cross-attention, the Decoder produces the Query, while the Encoder's output provides the Keys and Values[cite: 1238]."
    },
    {
        "topic": "Lec 9: Optimization",
        "text": "What is <b>PPO</b> (Proximal Policy Optimization)?",
        "options": [
            "An RL algorithm used to optimize the policy without large unstable updates",
            "A dataset for pretraining",
            "A type of transformer attention",
            "A method for tokenization"
        ],
        "correct": 0,
        "rationale": "PPO is the standard on-policy RL algorithm used in RLHF. It constrains the update step to prevent the policy from changing too drastically at once[cite: 3297]."
    }
    {
        "topic": "Lec 4: Language Models",
        "text": "What is the <b>Markov Assumption</b> in traditional N-gram language models?",
        "options": [
            "The probability of the next word depends only on a fixed window of previous words",
            "The probability of the next word depends on the entire history of the document",
            "The words are statistically independent of each other",
            "The future words influence the past words"
        ],
        "correct": 0,
        [cite_start]"rationale": "The Markov Assumption simplifies modeling by assuming the future ($x_{t+1}$) depends only on a short history ($x_{t-n+1}...x_t$), not the whole sequence[cite: 4]."
    },
    {
        "topic": "Lec 4: RNNs",
        "text": "Why is training standard RNNs difficult over long sequences?",
        "options": [
            "Gradients vanish or explode due to repeated matrix multiplication",
            "They require too much memory compared to Transformers",
            "They cannot handle variable length sequences",
            "They are restricted to fixed-size inputs"
        ],
        "correct": 0,
        "rationale": "In standard RNNs, the gradient flows through the same weight matrix $W_h$ repeatedly during backpropagation through time. [cite_start]If eigenvalues are < 1, gradients vanish; if > 1, they explode[cite: 29]."
    },
    {
        "topic": "Lec 4: LSTMs",
        "text": "Which component of an LSTM is primarily responsible for preventing Vanishing Gradients?",
        "options": [
            "The Forget Gate",
            "The Output Gate",
            "The Tanh Activation",
            "The Hidden State"
        ],
        "correct": 0,
        [cite_start]"rationale": "The Forget Gate allows the LSTM to pass the cell state $C_{t-1}$ through to $C_t$ with a weight near 1.0, creating a 'gradient highway' that prevents decay[cite: 32]."
    },
    {
        "topic": "Lec 4: Evaluation",
        "text": "What is <b>Perplexity</b> in language modeling?",
        "options": [
            "The inverse probability of the test set, normalized by the number of words",
            "The accuracy of the next token prediction",
            "The total cross-entropy loss",
            "The time it takes to generate a token"
        ],
        "correct": 0,
        "rationale": "Perplexity is defined as $PP(W) = P(w_1...w_N)^{-1/N}$. [cite_start]Intuitively, it represents the 'weighted average branching factor'â€”the number of words the model is confused between[cite: 17]."
    },
    {
        "topic": "Lec 6: SSL (MAE)",
        "text": "Why does Masked Autoencoding (MAE) use a very high masking ratio (e.g., 75%) compared to BERT (15%)?",
        "options": [
            "To force the model to learn holistic semantic representations rather than local interpolation",
            "To speed up training by processing fewer pixels",
            "Because images have less information than text",
            "To prevent the model from overfitting to colors"
        ],
        "correct": 0,
        "rationale": "Images have high spatial redundancy. If you mask only 15%, the model can easily solve the task by interpolating from neighbors without understanding the object. [cite_start]75% forces high-level understanding [cite: 13-14]."
    },
    {
        "topic": "Lec 6: SSL (MoCo)",
        "text": "In Momentum Contrast (MoCo), what is the purpose of the <b>Momentum Encoder</b>?",
        "options": [
            "To maintain a consistent dictionary of negative keys without doing backprop on them",
            "To speed up the query encoder updates",
            "To augment the images dynamically",
            "To calculate the classification loss"
        ],
        "correct": 0,
        "rationale": "MoCo uses a queue of keys encoded by a slowly updating (momentum) network. [cite_start]This allows a large stream of consistent negative samples without the cost of updating the key encoder via backprop every step[cite: 41]."
    },
    {
        "topic": "Lec 8: Prompting",
        "text": "What is <b>Chain-of-Thought (CoT)</b> prompting?",
        "options": [
            "Prompting the model to generate intermediate reasoning steps before the answer",
            "Connecting multiple models together in a chain",
            "Fine-tuning the model on logic puzzles",
            "Using a chain of if-else statements in the prompt"
        ],
        "correct": 0,
        [cite_start]"rationale": "CoT encourages the model to decompose complex problems into intermediate steps (e.g., 'Let's think step by step'), which significantly improves performance on reasoning tasks[cite: 19]."
    },
    {
        "topic": "Lec 8: Prompting",
        "text": "What is <b>Self-Consistency</b> in complex reasoning?",
        "options": [
            "Sampling multiple reasoning paths and taking a majority vote for the answer",
            "Forcing the model to check its own work",
            "Training the model to be consistent with human labels",
            "Using a temperature of 0.0 to ensure deterministic outputs"
        ],
        "correct": 0,
        [cite_start]"rationale": "Self-consistency involves sampling a diverse set of reasoning paths (using non-zero temperature) and selecting the most consistent final answer from the group[cite: 23]."
    },
    {
        "topic": "Lec 8: Prompting",
        "text": "What is the difference between <b>Soft Prompts</b> (Prompt Tuning) and <b>Hard Prompts</b>?",
        "options": [
            "Soft Prompts are learnable continuous vectors; Hard Prompts are discrete text tokens",
            "Soft Prompts are written by humans; Hard Prompts are written by AI",
            "Soft Prompts are used for generation; Hard Prompts for classification",
            "Soft Prompts update all model weights; Hard Prompts update none"
        ],
        "correct": 0,
        "rationale": "Hard prompts are discrete words (e.g., 'Translate:'). [cite_start]Soft prompts are learnable tensors prepended to the embedding layer that optimize task performance without changing model weights[cite: 39]."
    },
    {
        "topic": "Lec 8: Emergence",
        "text": "What are <b>Emergent Abilities</b> in Large Language Models?",
        "options": [
            "Abilities that appear suddenly only after the model reaches a certain scale",
            "Abilities that are explicitly programmed into the model",
            "Abilities that degrade as the model gets larger",
            "Abilities found only in small, efficient models"
        ],
        "correct": 0,
        [cite_start]"rationale": "Emergent abilities (like arithmetic or CoT) are not present in smaller models but appear discontinuously once the model parameters or training compute pass a critical threshold[cite: 46]."
    },
    {
        "topic": "Lec 10: RL Basics",
        "text": "What defines the <b>Markov Property</b> in Reinforcement Learning?",
        "options": [
            "The future depends only on the current state, not the history",
            "The future is completely random",
            "The state space must be discrete",
            "The reward is always immediate"
        ],
        "correct": 0,
        "rationale": "A state $S_t$ is Markov if it contains all relevant information from the history. [cite_start]Mathematically, $P(S_{t+1} | S_t) = P(S_{t+1} | S_1, ..., S_t)$[cite: 17]."
    },
    {
        "topic": "Lec 10: RL Basics",
        "text": "What is the difference between the <b>Value Function</b> $V(s)$ and the <b>Q-Function</b> $Q(s, a)$?",
        "options": [
            "V(s) is the value of a state; Q(s, a) is the value of taking action 'a' in state 's'",
            "V(s) determines the policy; Q(s, a) determines the reward",
            "V(s) is for continuous actions; Q(s, a) is for discrete actions",
            "They are identical terms for the same concept"
        ],
        "correct": 0,
        "rationale": "$V_{\\pi}(s)$ is the expected return starting from state $s$. [cite_start]$Q_{\\pi}(s,a)$ is the expected return starting from state $s$, taking action $a$, and then following policy $\\pi$[cite: 30]."
    },
    {
        "topic": "Lec 11: RL Algorithms",
        "text": "What characterizes <b>Off-Policy</b> learning (e.g., Q-Learning)?",
        "options": [
            "The agent learns from data generated by a different policy (e.g., old data)",
            "The agent only learns from data generated by the current policy",
            "The agent does not use a policy at all",
            "The agent learns without a reward function"
        ],
        "correct": 0,
        [cite_start]"rationale": "Off-policy algorithms can update the target policy using data collected by a behavior policy (often stored in a Replay Buffer), improving sample efficiency[cite: 17]."
    },
    {
        "topic": "Lec 11: Policy Gradient",
        "text": "What is the <b>Policy Gradient Theorem</b> used for?",
        "options": [
            "To estimate the gradient of the expected reward with respect to policy parameters",
            "To calculate the exact value of a state",
            "To ensure the policy always takes the greedy action",
            "To converting Q-values into V-values"
        ],
        "correct": 0,
        [cite_start]"rationale": "It provides a way to calculate $\\nabla J(\\theta)$ (gradient of performance) by sampling trajectories, enabling optimization of non-differentiable reward functions via gradient ascent[cite: 43]."
    },
    {
        "topic": "Lec 3: CNN Architectures",
        "text": "Why are sequences of 3x3 convolutions preferred over large single convolutions (e.g., 7x7)?",
        "options": [
            "They have fewer parameters and more non-linearities",
            "They cover a smaller receptive field",
            "They are faster to compute on CPUs",
            "They remove the need for pooling"
        ],
        "correct": 0,
        [cite_start]"rationale": "A stack of three 3x3 convs has the same receptive field as a 7x7 (7x7 pixels), but with fewer parameters ($3 \\times 3^2$ vs $7^2$) and more non-linear activations in between[cite: 55]."
    },
    {
        "topic": "Lec 5: Transformers",
        "text": "What is the purpose of the <b>Encoder</b> in the original Transformer architecture?",
        "options": [
            "To process the input sequence and generate context representations",
            "To generate the output sequence autoregressively",
            "To compress the input into a single vector",
            "To classify the sentiment of the text"
        ],
        "correct": 0,
        [cite_start]"rationale": "The Encoder maps the input sequence to a sequence of continuous representations ($z$), which holds the context for the Decoder to attend to[cite: 13]."
    },
    {
        "topic": "Lec 5: Efficiency",
        "text": "What is the computational bottleneck of the Transformer for very long sequences?",
        "options": [
            "The quadratic memory/time complexity of Self-Attention",
            "The size of the Feed Forward Network",
            "The number of layers in the model",
            "The embedding lookup table"
        ],
        "correct": 0,
        "rationale": "Self-attention requires computing an $N \\times N$ matrix. [cite_start]As sequence length $N$ increases, memory and compute grow by $N^2$, making long contexts expensive[cite: 98]."
    },
    {
        "topic": "Lec 9: RLHF",
        "text": "In RLHF, why do we typically freeze the SFT model parameters when initializing the Reference Model?",
        "options": [
            "To provide a stable baseline for calculating the KL penalty",
            "To save memory on the GPU",
            "Because the SFT model is already perfect",
            "To prevent the reward model from changing"
        ],
        "correct": 0,
        "rationale": "The Reference Model ($p^{PT}$) is usually the static SFT model. [cite_start]We compare the active RL policy against it to compute the KL divergence penalty[cite: 36]."
    },
    {
        "topic": "Lec 11: Exploration",
        "text": "What is the <b>Epsilon-Greedy</b> strategy?",
        "options": [
            "Choosing a random action with probability epsilon, and the best action otherwise",
            "Always choosing the best action",
            "Always choosing a random action",
            "Choosing actions based on a softmax distribution"
        ],
        "correct": 0,
        [cite_start]"rationale": "Epsilon-greedy is a simple exploration strategy: with probability $\\epsilon$ explore (random action), and with probability $1-\\epsilon$ exploit (greedy action)[cite: 22]."
    },
    {
        "topic": "Lec 6: SSL",
        "text": "What is a <b>Pretext Task</b> in Self-Supervised Learning?",
        "options": [
            "A task designed to force the model to learn features without human labels",
            "The final downstream classification task",
            "A task performed by human annotators",
            "A method to clean the dataset"
        ],
        "correct": 0,
        [cite_start]"rationale": "Pretext tasks (like predicting the next word, or rotation prediction) are auto-generated from the data itself to provide a training signal for learning representations[cite: 5]."
    },
    {
        "topic": "Lec 9: DPO",
        "text": "In DPO, how is the 'implicit reward' calculated?",
        "options": [
            "Using the log-ratio of the policy likelihood to the reference likelihood",
            "Using a separate neural network regressor",
            "Using the BLEU score of the output",
            "Using the squared error of the prediction"
        ],
        "correct": 0,
        "rationale": "DPO derives the reward implicitly as $R(x,y) = \\beta \\log (\\pi(y|x) / \\pi_{ref}(y|x))$. [cite_start]It uses the model's own probabilities as the reward signal[cite: 53]."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "What is <b>Translation Equivariance</b>?",
        "options": [
            "If the input shifts, the output feature map shifts by the same amount",
            "If the input shifts, the output remains exactly the same",
            "If the input rotates, the output rotates",
            "The output is independent of the input position"
        ],
        "correct": 0,
        "rationale": "Equivariance means $f(g(x)) = g(f(x))$. [cite_start]For conv layers, shifting the input image shifts the activation map correspondingly (unlike Invariance, which is achieved by pooling)[cite: 22]."
    },
    {
        "topic": "Lec 5: Positional Encoding",
        "text": "Why does the Sinusoidal Positional Encoding use different frequencies?",
        "options": [
            "To allow the model to attend to relative positions at different scales",
            "To make the encoding look like a wave",
            "To compress the input data",
            "To encrypt the token positions"
        ],
        "correct": 0,
        [cite_start]"rationale": "Using a geometric progression of frequencies allows the model to easily learn to attend by relative positions (linear functions of phase shifts)[cite: 24]."
    },
    {
        "topic": "Lec 9: Optimization",
        "text": "Why is <b>AdamW</b> preferred over Adam for training LLMs?",
        "options": [
            "It decouples weight decay from the gradient update",
            "It has a faster learning rate",
            "It uses less memory",
            "It removes the need for momentum"
        ],
        "correct": 0,
        "rationale": "Standard Adam implements L2 regularization incorrectly. [cite_start]AdamW decouples weight decay, applying it directly to the weights, which improves generalization[cite: 39]."
    },
    {
        "topic": "Lec 10: RL Basics",
        "text": "What is the <b>Credit Assignment Problem</b> in RL?",
        "options": [
            "Determining which past action is responsible for a current reward",
            "Determining how much to pay the human labelers",
            "Assigning weights to the neural network layers",
            "Deciding which state to start in"
        ],
        "correct": 0,
        "rationale": "In RL, rewards are often sparse and delayed. [cite_start]Credit assignment is the challenge of figuring out *which* of the many actions taken previously actually caused the reward[cite: 8]."
    },
    {
        "topic": "Lec 5: Attention",
        "text": "What is <b>Global Attention</b> vs <b>Local Attention</b>?",
        "options": [
            "Global attends to all tokens; Local attends to a window of neighbors",
            "Global is for training; Local is for inference",
            "Global uses all heads; Local uses one head",
            "Global is slow; Local is inaccurate"
        ],
        "correct": 0,
        "rationale": "Global attention (standard) computes interactions between all pairs. [cite_start]Local attention (like Sliding Window) restricts interactions to nearby tokens to save compute[cite: 98]."
    },
    {
        "topic": "Lec 9: Alignment",
        "text": "What is the 'Helpful vs Harmless' trade-off?",
        "options": [
            "Models that refuse to answer unsafe questions might become less helpful",
            "Harmless models are always helpful",
            "Helpful models are always harmless",
            "There is no trade-off"
        ],
        "correct": 0,
        [cite_start]"rationale": "Aggressive safety filtering (Harmlessness) can lead to 'false refusals' where the model refuses to answer benign questions, reducing Helpfulness[cite: 24]."
    },
    {
        "topic": "Lec 4: RNNs",
        "text": "What is <b>Backpropagation Through Time (BPTT)</b>?",
        "options": [
            "Unrolling the RNN over time steps and applying backprop",
            "Training the RNN in reverse order",
            "Predicting the future from the past",
            "Using a transformer to train an RNN"
        ],
        "correct": 0,
        [cite_start]"rationale": "BPTT involves 'unrolling' the recurrent loop into a deep feedforward network structure and backpropagating errors through the time steps[cite: 54]."
    },
    {
        "topic": "Lec 11: RL Algorithms",
        "text": "What is the main benefit of using a <b>Replay Buffer</b> in DQN?",
        "options": [
            "It breaks correlations between consecutive samples",
            "It increases the reward",
            "It makes the model on-policy",
            "It removes the need for a target network"
        ],
        "correct": 0,
        "rationale": "Consecutive samples in RL are highly correlated. [cite_start]Storing them in a buffer and sampling randomly breaks these correlations, stabilizing training[cite: 32]."
    },
    {
        "topic": "Lec 9: Data",
        "text": "What is the 'Digital Sweatshop' concern in RLHF?",
        "options": [
            "Reliance on low-wage overseas workers for labeling",
            "Using AI to label data",
            "Training models on GPUs",
            "The high cost of electricity"
        ],
        "correct": 0,
        [cite_start]"rationale": "Much of the human preference data for RLHF is generated by workers in the Global South paid very low wages, raising ethical concerns[cite: 57]."
    },
    {
        "topic": "Lec 8: Prompting",
        "text": "What is <b>Least-to-Most</b> prompting?",
        "options": [
            "Breaking a problem into subproblems and solving them sequentially",
            "Starting with the easiest examples in the dataset",
            "Asking the model to sort the answer",
            "Using the smallest model first"
        ],
        "correct": 0,
        "rationale": "Least-to-Most prompting explicitly asks the model to decompose a complex problem into sub-questions and solve them in order to build up the final answer."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "What happens if you use a Stride of 1 and 'Same' Padding?",
        "options": [
            "The output spatial dimension remains the same as the input",
            "The output is smaller",
            "The output is larger",
            "The image is flipped"
        ],
        "correct": 0,
        [cite_start]"rationale": "'Same' padding adds enough zeros to ensure that with a stride of 1, the output width/height matches the input width/height[cite: 97]."
    },
    {
        "topic": "Lec 5: Attention",
        "text": "What is the <b>Query</b> vector conceptually analogous to?",
        "options": [
            "What the current token is looking for",
            "What the current token contains",
            "The actual content to be retrieved",
            "The position of the token"
        ],
        "correct": 0,
        "rationale": "The Query represents the current token's 'search intent'. It is compared against Keys to find relevant information."
    },
    {
        "topic": "Lec 9: Models",
        "text": "What distinguishes <b>CodeLlama</b> from standard Llama?",
        "options": [
            "It is finetuned specifically on code datasets",
            "It uses a different architecture",
            "It is a closed source model",
            "It only understands Python"
        ],
        "correct": 0,
        [cite_start]"rationale": "CodeLlama is a version of Llama further pre-trained and fine-tuned on code-specific datasets to enhance programming capabilities[cite: 104]."
    },
    {
        "topic": "Lec 6: SSL",
        "text": "In Contrastive Learning, what is the role of <b>Negative Pairs</b>?",
        "options": [
            "To prevent the model from outputting a constant solution (Collapse)",
            "To increase the dataset size",
            "To make the model learn colors",
            "To reduce the training time"
        ],
        "correct": 0,
        "rationale": "Without negatives, the loss is minimized by making all outputs identical. [cite_start]Negatives force the model to push dissimilar items apart in the embedding space[cite: 20]."
    }
]