[
    {
        "topic": "Exam Priority: Regression",
        "text": "<b>[Must Know]</b> What is the Loss Function for <b>Linear Regression</b> vs <b>Logistic Regression</b>?",
        "options": [
            "Linear: Mean Squared Error (MSE) | Logistic: Binary Cross Entropy",
            "Linear: Cross Entropy | Logistic: MSE",
            "Linear: Hinge Loss | Logistic: Kullback-Leibler",
            "Linear: MSE | Logistic: MSE"
        ],
        "correct": 0,
        "rationale": "Linear regression predicts real values (output -inf to +inf) using MSE. Logistic predicts probabilities (0 to 1) using Binary Cross Entropy[cite: 1]."
    },
    {
        "topic": "Exam Priority: CNNs",
        "text": "<b>[Must Know]</b> Compare Convolutional layers and Fully Connected (FC) layers in terms of <b>Parameter Efficiency</b>.",
        "options": [
            "Conv layers are more efficient due to Weight Sharing and Sparse Connectivity",
            "FC layers are more efficient because they use simple matrix multiplication",
            "Conv layers use more parameters because of the filters",
            "They have roughly the same efficiency"
        ],
        "correct": 0,
        "rationale": "A Conv filter reuses the same parameters across the entire image (Weight Sharing), whereas an FC layer requires a unique weight for every single input-output connection[cite: 1]."
    },
    {
        "topic": "Exam Priority: Transformers",
        "text": "<b>[Must Know]</b> What is the specific role of <b>Layer Normalization</b> in Transformers vs Batch Norm?",
        "options": [
            "LN normalizes across the feature dimension for each token (independent of batch size)",
            "LN normalizes across the batch dimension (dependent on batch size)",
            "LN adds noise to the training process",
            "LN is only used in the Decoder"
        ],
        "correct": 0,
        "rationale": "Batch Norm is unstable in NLP because sentence lengths vary and batches are small. Layer Norm works per-sample, making it robust for sequence modeling[cite: 1]."
    },
    {
        "topic": "Exam Priority: Transformers",
        "text": "<b>[Must Know]</b> What is the purpose of <b>Residual Connections</b> in Transformers?",
        "options": [
            "To allow gradients to flow through the network (solving vanishing gradients)",
            "To compress the model size",
            "To increase the non-linearity",
            "To combine the Query and Key vectors"
        ],
        "correct": 0,
        "rationale": "Residuals ($x + f(x)$) create a 'highway' for gradients to propagate backwards to early layers without being attenuated by non-linearities[cite: 1]."
    },
    {
        "topic": "Exam Priority: SSL",
        "text": "<b>[Must Know]</b> Compare <b>MAE</b> (Masked Autoencoding) and <b>Contrastive Learning</b> goals.",
        "options": [
            "MAE: Reconstruction (predict pixels) | Contrastive: Discrimination (distinguish views)",
            "MAE: Discrimination | Contrastive: Reconstruction",
            "MAE: Clustering | Contrastive: Generation",
            "MAE: RLHF | Contrastive: Supervised"
        ],
        "correct": 0,
        "rationale": "MAE masks parts of the input and tries to rebuild them (Generative/Local features). Contrastive learning pulls augmentations of the same image together (Discriminative/Global features)[cite: 1]."
    },
    {
        "topic": "Exam Priority: SSL",
        "text": "<b>[Must Know]</b> In <b>Contrastive Learning</b>, how do we construct a 'Positive Pair'?",
        "options": [
            "Two different augmentations (crops/colors) of the same image",
            "An image and its label",
            "Two images from the same class",
            "An image and a random patch"
        ],
        "correct": 0,
        "rationale": "A positive pair consists of two views of the same source instance. Negative pairs are views of *different* instances[cite: 1]."
    },
    {
        "topic": "Exam Priority: SSL",
        "text": "<b>[Must Know]</b> How does Masking help in <b>NLP</b> vs <b>CV</b>?",
        "options": [
            "NLP: Context understanding (Bidirectional) | CV: Feature representation (Global structure)",
            "NLP: Grammar correction | CV: Color correction",
            "NLP: Text generation | CV: Image generation",
            "NLP: None | CV: None"
        ],
        "correct": 0,
        "rationale": "In NLP (BERT), masking forces the model to use bidirectional context. In CV (MAE), high masking ratios force the model to learn global shapes rather than local textures[cite: 1]."
    },
    {
        "topic": "Exam Priority: Prompting",
        "text": "<b>[Must Know]</b> Compare <b>Soft Prompting</b> vs <b>Hard Prompting</b> regarding optimization.",
        "options": [
            "Soft: Continuous differentiable embeddings | Hard: Discrete text tokens",
            "Soft: Human readable | Hard: Machine code",
            "Soft: Discrete tokens | Hard: Continuous embeddings",
            "Soft: Updates model weights | Hard: Freezes model weights"
        ],
        "correct": 0,
        "rationale": "Soft prompts are learnable vectors added to the input embedding space (differentiable). Hard prompts are fixed text strings chosen by humans (non-differentiable)[cite: 1]."
    },
    {
        "topic": "Exam Priority: ICL",
        "text": "<b>[Must Know]</b> What is a major weakness of <b>In-Context Learning (ICL)</b>?",
        "options": [
            "No performance guarantee & sensitive to prompt format/order",
            "Requires expensive gradient updates",
            "Requires a huge dataset for fine-tuning",
            "Cannot handle few-shot examples"
        ],
        "correct": 0,
        "rationale": "ICL does not update weights, so it can be unstable—changing the order of examples in the prompt can drastically change the output. It also has limited context length[cite: 1]."
    },
    {
        "topic": "Exam Priority: Tuning",
        "text": "<b>[Must Know]</b> How does <b>Instruction Tuning</b> differ from <b>In-Context Learning</b> regarding model updates?",
        "options": [
            "Instruction Tuning updates weights; ICL does not",
            "ICL updates weights; Instruction Tuning does not",
            "Both update weights",
            "Neither updates weights"
        ],
        "correct": 0,
        "rationale": "Instruction tuning is a form of Supervised Fine-Tuning (gradient descent). ICL is purely inference-time conditioning[cite: 1]."
    },
    {
        "topic": "Exam Priority: Scaling",
        "text": "<b>[Must Know]</b> How do <b>Scaling Laws</b> guide resource allocation?",
        "options": [
            "They identify the optimal balance between Model Size, Dataset Size, and Compute",
            "They tell us to always make the model bigger",
            "They tell us to always add more data",
            "They predict the exact loss for any architecture"
        ],
        "correct": 0,
        "rationale": "Scaling laws (Chinchilla) allow researchers to predict performance and allocate budget efficiently, avoiding 'oversizing' the model relative to the data[cite: 1]."
    },
    {
        "topic": "Exam Priority: RL",
        "text": "<b>[Must Know]</b> Why do Gradient methods in RL often have <b>High Variance</b>?",
        "options": [
            "Due to the high variance of rewards in the trajectory",
            "Because the learning rate is too high",
            "Because the models are too deep",
            "Because of the KL penalty"
        ],
        "correct": 0,
        "rationale": "Monte Carlo estimates of the gradient rely on sampling entire trajectories. The rewards along these trajectories can vary wildly, leading to noisy gradients[cite: 1]."
    },
    {
        "topic": "Exam Priority: RL",
        "text": "<b>[Must Know]</b> How do we achieve <b>Variance Reduction</b> in RL Policy Gradients?",
        "options": [
            "By subtracting a Baseline from the reward",
            "By increasing the batch size",
            "By using a larger model",
            "By removing the discount factor"
        ],
        "correct": 0,
        "rationale": "Subtracting a baseline (like the average reward) centers the returns, reducing the variance of the gradient estimator without introducing bias[cite: 1]."
    },
    {
        "topic": "Exam Priority: Alignment",
        "text": "<b>[Must Know]</b> What is the mathematical advantage of <b>DPO</b> over RLHF?",
        "options": [
            "It optimizes the policy analytically without a separate Reward Model",
            "It uses a more accurate Reward Model",
            "It allows for larger batch sizes",
            "It does not require preference data"
        ],
        "correct": 0,
        "rationale": "DPO uses the Bradley-Terry model to derive a closed-form solution for the optimal policy, allowing direct optimization on preference pairs[cite: 1]."
    },
    {
        "topic": "Exam Priority: RL",
        "text": "<b>[Must Know]</b> Compare <b>Sample Efficiency</b>: Model-Based vs. On-Policy vs. Off-Policy.",
        "options": [
            "Model-Based > Off-Policy > On-Policy",
            "On-Policy > Off-Policy > Model-Based",
            "Off-Policy > Model-Based > On-Policy",
            "They are roughly equal"
        ],
        "correct": 0,
        "rationale": "Model-Based is most efficient (simulates world). Off-Policy is medium (reuses data). On-Policy (PPO) is least efficient (throws away data after update)[cite: 1]."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "What is the receptive field of a neuron in the first layer (3x3 kernel)?",
        "options": [
            "3x3 pixels",
            "1x1 pixels",
            "The whole image",
            "Depends on Stride"
        ],
        "correct": 0,
        "rationale": "In the first layer, a neuron only sees the pixels immediately under its filter, so 3x3."
    },
    {
        "topic": "Lec 5: Transformers",
        "text": "How does the 'Scaled Dot-Product Attention' handle the magnitude of dot products?",
        "options": [
            "It divides by the square root of the dimension $$ \\sqrt{d_k} $$",
            "It uses a Tanh activation function",
            "It applies Batch Normalization",
            "It clips gradients to 1.0"
        ],
        "correct": 0,
        "rationale": "As dimensions ($d_k$) increase, dot products grow large, pushing Softmax into regions with extremely small gradients. Scaling by $1/\\sqrt{d_k}$ counteracts this."
    },
    {
        "topic": "Lec 5: Positional Encoding",
        "text": "What is <b>RoPE</b> (Rotary Positional Embeddings)?",
        "options": [
            "A relative encoding that uses rotation matrices to encode position",
            "An absolute encoding using sinusoidal waves",
            "A learned embedding layer added to tokens",
            "A method to shuffle tokens randomly"
        ],
        "correct": 0,
        "rationale": "RoPE (used in LLaMA) encodes relative positions by rotating the query and key vectors in the embedding space, allowing the model to generalize to sequence lengths longer than seen during training."
    },
    {
        "topic": "Lec 9: Instruct Tuning",
        "text": "What did the <b>LIMA</b> (Less Is More for Alignment) paper demonstrate?",
        "options": [
            "1,000 high-quality instruction examples are sufficient for strong alignment",
            "You need millions of instruction examples to align a model",
            "RLHF is strictly necessary for all alignment tasks",
            "Pre-training data quality does not matter"
        ],
        "correct": 0,
        "rationale": "LIMA showed that the 'Superficial Alignment Hypothesis' is true: the model learns knowledge during pre-training, and instruction tuning just teaches it the specific format/style of interaction."
    },
    {
        "topic": "Lec 9: RLHF",
        "text": "In the standard RLHF pipeline (InstructGPT), what is the function of the <b>KL Penalty</b>?",
        "options": [
            "To prevent the RL policy from drifting too far from the initial SFT model",
            "To force the model to output shorter sentences",
            "To increase the diversity of the generated text",
            "To improve the accuracy of the Reward Model"
        ],
        "correct": 0,
        "rationale": "Without the KL penalty, the RL algorithm would exploit flaws in the reward model (Reward Hacking) and output gibberish that scores high rewards. The penalty keeps it grounded to intelligible language."
    },
    {
        "topic": "Lec 9: Data",
        "text": "What is a 'Rejection Sampling' approach in LLM fine-tuning (used in Llama 2)?",
        "options": [
            "Sampling K outputs, using the Reward Model to pick the best, and fine-tuning on that",
            "Throwing away data that is too long",
            "Stopping training when loss increases",
            "Using human labelers to rewrite bad answers"
        ],
        "correct": 0,
        "rationale": "Rejection Sampling (or Best-of-N) involves generating multiple candidates from the current policy, ranking them with the reward model, and treating the 'winner' as a new gold-standard label for supervised training."
    },
    {
        "topic": "Lec 4: Language Models",
        "text": "What is the <b>Markov Assumption</b> in traditional N-gram language models?",
        "options": [
            "The probability of the next word depends only on a fixed window of previous words",
            "The probability of the next word depends on the entire history of the document",
            "The words are statistically independent of each other",
            "The future words influence the past words"
        ],
        "correct": 0,
        "rationale": "The Markov Assumption simplifies modeling by assuming the future ($x_{t+1}$) depends only on a short history ($x_{t-n+1}...x_t$), not the whole sequence."
    },
    {
        "topic": "Lec 4: RNNs",
        "text": "Why is training standard RNNs difficult over long sequences?",
        "options": [
            "Gradients vanish or explode due to repeated matrix multiplication",
            "They require too much memory compared to Transformers",
            "They cannot handle variable length sequences",
            "They are restricted to fixed-size inputs"
        ],
        "correct": 0,
        "rationale": "In standard RNNs, the gradient flows through the same weight matrix $W_h$ repeatedly during backpropagation through time. If eigenvalues are < 1, gradients vanish; if > 1, they explode."
    },
    {
        "topic": "Lec 4: LSTMs",
        "text": "Which component of an LSTM is primarily responsible for preventing Vanishing Gradients?",
        "options": [
            "The Forget Gate",
            "The Output Gate",
            "The Tanh Activation",
            "The Hidden State"
        ],
        "correct": 0,
        "rationale": "The Forget Gate allows the LSTM to pass the cell state $C_{t-1}$ through to $C_t$ with a weight near 1.0, creating a 'gradient highway' that prevents decay."
    },
    {
        "topic": "Lec 4: Evaluation",
        "text": "What is <b>Perplexity</b> in language modeling?",
        "options": [
            "The inverse probability of the test set, normalized by the number of words",
            "The accuracy of the next token prediction",
            "The total cross-entropy loss",
            "The time it takes to generate a token"
        ],
        "correct": 0,
        "rationale": "Perplexity is defined as $PP(W) = P(w_1...w_N)^{-1/N}$. Intuitively, it represents the 'weighted average branching factor'—the number of words the model is confused between."
    },
    {
        "topic": "Lec 6: SSL (MAE)",
        "text": "Why does Masked Autoencoding (MAE) use a very high masking ratio (e.g., 75%) compared to BERT (15%)?",
        "options": [
            "To force the model to learn holistic semantic representations rather than local interpolation",
            "To speed up training by processing fewer pixels",
            "Because images have less information than text",
            "To prevent the model from overfitting to colors"
        ],
        "correct": 0,
        "rationale": "Images have high spatial redundancy. If you mask only 15%, the model can easily solve the task by interpolating from neighbors without understanding the object. 75% forces high-level understanding."
    },
    {
        "topic": "Lec 6: SSL (MoCo)",
        "text": "In Momentum Contrast (MoCo), what is the purpose of the <b>Momentum Encoder</b>?",
        "options": [
            "To maintain a consistent dictionary of negative keys without doing backprop on them",
            "To speed up the query encoder updates",
            "To augment the images dynamically",
            "To calculate the classification loss"
        ],
        "correct": 0,
        "rationale": "MoCo uses a queue of keys encoded by a slowly updating (momentum) network. This allows a large stream of consistent negative samples without the cost of updating the key encoder via backprop every step."
    },
    {
        "topic": "Lec 8: Prompting",
        "text": "What is <b>Chain-of-Thought (CoT)</b> prompting?",
        "options": [
            "Prompting the model to generate intermediate reasoning steps before the answer",
            "Connecting multiple models together in a chain",
            "Fine-tuning the model on logic puzzles",
            "Using a chain of if-else statements in the prompt"
        ],
        "correct": 0,
        "rationale": "CoT encourages the model to decompose complex problems into intermediate steps (e.g., 'Let's think step by step'), which significantly improves performance on reasoning tasks."
    },
    {
        "topic": "Lec 8: Prompting",
        "text": "What is <b>Self-Consistency</b> in complex reasoning?",
        "options": [
            "Sampling multiple reasoning paths and taking a majority vote for the answer",
            "Forcing the model to check its own work",
            "Training the model to be consistent with human labels",
            "Using a temperature of 0.0 to ensure deterministic outputs"
        ],
        "correct": 0,
        "rationale": "Self-consistency involves sampling a diverse set of reasoning paths (using non-zero temperature) and selecting the most consistent final answer from the group."
    },
    {
        "topic": "Lec 8: Emergence",
        "text": "What are <b>Emergent Abilities</b> in Large Language Models?",
        "options": [
            "Abilities that appear suddenly only after the model reaches a certain scale",
            "Abilities that are explicitly programmed into the model",
            "Abilities that degrade as the model gets larger",
            "Abilities found only in small, efficient models"
        ],
        "correct": 0,
        "rationale": "Emergent abilities (like arithmetic or CoT) are not present in smaller models but appear discontinuously once the model parameters or training compute pass a critical threshold."
    },
    {
        "topic": "Lec 10: RL Basics",
        "text": "What defines the <b>Markov Property</b> in Reinforcement Learning?",
        "options": [
            "The future depends only on the current state, not the history",
            "The future is completely random",
            "The state space must be discrete",
            "The reward is always immediate"
        ],
        "correct": 0,
        "rationale": "A state $S_t$ is Markov if it contains all relevant information from the history. Mathematically, $P(S_{t+1} | S_t) = P(S_{t+1} | S_1, ..., S_t)$."
    },
    {
        "topic": "Lec 10: RL Basics",
        "text": "What is the difference between the <b>Value Function</b> $V(s)$ and the <b>Q-Function</b> $Q(s, a)$?",
        "options": [
            "V(s) is the value of a state; Q(s, a) is the value of taking action 'a' in state 's'",
            "V(s) determines the policy; Q(s, a) determines the reward",
            "V(s) is for continuous actions; Q(s, a) is for discrete actions",
            "They are identical terms for the same concept"
        ],
        "correct": 0,
        "rationale": "$V_{\\pi}(s)$ is the expected return starting from state $s$. $Q_{\\pi}(s,a)$ is the expected return starting from state $s$, taking action $a$, and then following policy $\\pi$."
    },
    {
        "topic": "Lec 11: RL Algorithms",
        "text": "What characterizes <b>Off-Policy</b> learning (e.g., Q-Learning)?",
        "options": [
            "The agent learns from data generated by a different policy (e.g., old data)",
            "The agent only learns from data generated by the current policy",
            "The agent does not use a policy at all",
            "The agent learns without a reward function"
        ],
        "correct": 0,
        "rationale": "Off-policy algorithms can update the target policy using data collected by a behavior policy (often stored in a Replay Buffer), improving sample efficiency."
    },
    {
        "topic": "Lec 11: Policy Gradient",
        "text": "What is the <b>Policy Gradient Theorem</b> used for?",
        "options": [
            "To estimate the gradient of the expected reward with respect to policy parameters",
            "To calculate the exact value of a state",
            "To ensure the policy always takes the greedy action",
            "To converting Q-values into V-values"
        ],
        "correct": 0,
        "rationale": "It provides a way to calculate $\\nabla J(\\theta)$ (gradient of performance) by sampling trajectories, enabling optimization of non-differentiable reward functions via gradient ascent."
    },
    {
        "topic": "Lec 3: CNN Architectures",
        "text": "Why are sequences of 3x3 convolutions preferred over large single convolutions (e.g., 7x7)?",
        "options": [
            "They have fewer parameters and more non-linearities",
            "They cover a smaller receptive field",
            "They are faster to compute on CPUs",
            "They remove the need for pooling"
        ],
        "correct": 0,
        "rationale": "A stack of three 3x3 convs has the same receptive field as a 7x7 (7x7 pixels), but with fewer parameters ($3 \\times 3^2$ vs $7^2$) and more non-linear activations in between."
    },
    {
        "topic": "Lec 5: Efficiency",
        "text": "What is the computational bottleneck of the Transformer for very long sequences?",
        "options": [
            "The quadratic memory/time complexity of Self-Attention",
            "The size of the Feed Forward Network",
            "The number of layers in the model",
            "The embedding lookup table"
        ],
        "correct": 0,
        "rationale": "Self-attention requires computing an $N \\times N$ matrix. As sequence length $N$ increases, memory and compute grow by $N^2$, making long contexts expensive."
    },
    {
        "topic": "Lec 9: RLHF",
        "text": "In RLHF, why do we typically freeze the SFT model parameters when initializing the Reference Model?",
        "options": [
            "To provide a stable baseline for calculating the KL penalty",
            "To save memory on the GPU",
            "Because the SFT model is already perfect",
            "To prevent the reward model from changing"
        ],
        "correct": 0,
        "rationale": "The Reference Model ($p^{PT}$) is usually the static SFT model. We compare the active RL policy against it to compute the KL divergence penalty."
    },
    {
        "topic": "Lec 11: Exploration",
        "text": "What is the <b>Epsilon-Greedy</b> strategy?",
        "options": [
            "Choosing a random action with probability epsilon, and the best action otherwise",
            "Always choosing the best action",
            "Always choosing a random action",
            "Choosing actions based on a softmax distribution"
        ],
        "correct": 0,
        "rationale": "Epsilon-greedy is a simple exploration strategy: with probability $\\epsilon$ explore (random action), and with probability $1-\\epsilon$ exploit (greedy action)."
    },
    {
        "topic": "Lec 6: SSL",
        "text": "What is a <b>Pretext Task</b> in Self-Supervised Learning?",
        "options": [
            "A task designed to force the model to learn features without human labels",
            "The final downstream classification task",
            "A task performed by human annotators",
            "A method to clean the dataset"
        ],
        "correct": 0,
        "rationale": "Pretext tasks (like predicting the next word, or rotation prediction) are auto-generated from the data itself to provide a training signal for learning representations."
    },
    {
        "topic": "Lec 9: DPO",
        "text": "In DPO, how is the 'implicit reward' calculated?",
        "options": [
            "Using the log-ratio of the policy likelihood to the reference likelihood",
            "Using a separate neural network regressor",
            "Using the BLEU score of the output",
            "Using the squared error of the prediction"
        ],
        "correct": 0,
        "rationale": "DPO derives the reward implicitly as $R(x,y) = \\beta \\log (\\pi(y|x) / \\pi_{ref}(y|x))$. It uses the model's own probabilities as the reward signal."
    },
    {
        "topic": "Lec 5: Positional Encoding",
        "text": "Why does the Sinusoidal Positional Encoding use different frequencies?",
        "options": [
            "To allow the model to attend to relative positions at different scales",
            "To make the encoding look like a wave",
            "To compress the input data",
            "To encrypt the token positions"
        ],
        "correct": 0,
        "rationale": "Using a geometric progression of frequencies allows the model to easily learn to attend by relative positions (linear functions of phase shifts)."
    },
    {
        "topic": "Lec 9: Optimization",
        "text": "Why is <b>AdamW</b> preferred over Adam for training LLMs?",
        "options": [
            "It decouples weight decay from the gradient update",
            "It has a faster learning rate",
            "It uses less memory",
            "It removes the need for momentum"
        ],
        "correct": 0,
        "rationale": "Standard Adam implements L2 regularization incorrectly. AdamW decouples weight decay, applying it directly to the weights, which improves generalization."
    },
    {
        "topic": "Lec 10: RL Basics",
        "text": "What is the <b>Credit Assignment Problem</b> in RL?",
        "options": [
            "Determining which past action is responsible for a current reward",
            "Determining how much to pay the human labelers",
            "Assigning weights to the neural network layers",
            "Deciding which state to start in"
        ],
        "correct": 0,
        "rationale": "In RL, rewards are often sparse and delayed. Credit assignment is the challenge of figuring out *which* of the many actions taken previously actually caused the reward."
    },
    {
        "topic": "Lec 5: Attention",
        "text": "What is <b>Global Attention</b> vs <b>Local Attention</b>?",
        "options": [
            "Global attends to all tokens; Local attends to a window of neighbors",
            "Global is for training; Local is for inference",
            "Global uses all heads; Local uses one head",
            "Global is slow; Local is inaccurate"
        ],
        "correct": 0,
        "rationale": "Global attention (standard) computes interactions between all pairs. Local attention (like Sliding Window) restricts interactions to nearby tokens to save compute."
    },
    {
        "topic": "Lec 9: Alignment",
        "text": "What is the 'Helpful vs Harmless' trade-off?",
        "options": [
            "Models that refuse to answer unsafe questions might become less helpful",
            "Harmless models are always helpful",
            "Helpful models are always harmless",
            "There is no trade-off"
        ],
        "correct": 0,
        "rationale": "Aggressive safety filtering (Harmlessness) can lead to 'false refusals' where the model refuses to answer benign questions, reducing Helpfulness."
    },
    {
        "topic": "Lec 4: RNNs",
        "text": "What is <b>Backpropagation Through Time (BPTT)</b>?",
        "options": [
            "Unrolling the RNN over time steps and applying backprop",
            "Training the RNN in reverse order",
            "Predicting the future from the past",
            "Using a transformer to train an RNN"
        ],
        "correct": 0,
        "rationale": "BPTT involves 'unrolling' the recurrent loop into a deep feedforward network structure and backpropagating errors through the time steps."
    },
    {
        "topic": "Lec 11: RL Algorithms",
        "text": "What is the main benefit of using a <b>Replay Buffer</b> in DQN?",
        "options": [
            "It breaks correlations between consecutive samples",
            "It increases the reward",
            "It makes the model on-policy",
            "It removes the need for a target network"
        ],
        "correct": 0,
        "rationale": "Consecutive samples in RL are highly correlated. Storing them in a buffer and sampling randomly breaks these correlations, stabilizing training."
    },
    {
        "topic": "Lec 9: Data",
        "text": "What is the 'Digital Sweatshop' concern in RLHF?",
        "options": [
            "Reliance on low-wage overseas workers for labeling",
            "Using AI to label data",
            "Training models on GPUs",
            "The high cost of electricity"
        ],
        "correct": 0,
        "rationale": "Much of the human preference data for RLHF is generated by workers in the Global South paid very low wages, raising ethical concerns."
    },
    {
        "topic": "Lec 8: Prompting",
        "text": "What is <b>Least-to-Most</b> prompting?",
        "options": [
            "Breaking a problem into subproblems and solving them sequentially",
            "Starting with the easiest examples in the dataset",
            "Asking the model to sort the answer",
            "Using the smallest model first"
        ],
        "correct": 0,
        "rationale": "Least-to-Most prompting explicitly asks the model to decompose a complex problem into sub-questions and solve them in order to build up the final answer."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "What happens if you use a Stride of 1 and 'Same' Padding?",
        "options": [
            "The output spatial dimension remains the same as the input",
            "The output is smaller",
            "The output is larger",
            "The image is flipped"
        ],
        "correct": 0,
        "rationale": "'Same' padding adds enough zeros to ensure that with a stride of 1, the output width/height matches the input width/height."
    },
    {
        "topic": "Lec 5: Attention",
        "text": "What is the <b>Query</b> vector conceptually analogous to?",
        "options": [
            "What the current token is looking for",
            "What the current token contains",
            "The actual content to be retrieved",
            "The position of the token"
        ],
        "correct": 0,
        "rationale": "The Query represents the current token's 'search intent'. It is compared against Keys to find relevant information."
    },
    {
        "topic": "Lec 9: Models",
        "text": "What distinguishes <b>CodeLlama</b> from standard Llama?",
        "options": [
            "It is finetuned specifically on code datasets",
            "It uses a different architecture",
            "It is a closed source model",
            "It only understands Python"
        ],
        "correct": 0,
        "rationale": "CodeLlama is a version of Llama further pre-trained and fine-tuned on code-specific datasets to enhance programming capabilities."
    },
    {
        "topic": "Lec 6: SSL",
        "text": "In Contrastive Learning, what is the role of <b>Negative Pairs</b>?",
        "options": [
            "To prevent the model from outputting a constant solution (Collapse)",
            "To increase the dataset size",
            "To make the model learn colors",
            "To reduce the training time"
        ],
        "correct": 0,
        "rationale": "Without negatives, the loss is minimized by making all outputs identical. Negatives force the model to push dissimilar items apart in the embedding space."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "What is the receptive field of a single neuron in the <b>first</b> conv layer with a 3x3 kernel?",
        "options": ["3x3 pixels", "1x1 pixels", "The whole image", "Depends on Stride"],
        "correct": 0,
        "rationale": "In the first layer, a neuron only sees the pixels immediately under its filter, so 3x3."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "If you have 10 filters of size 5x5x3, how many <b>biases</b> do you learn?",
        "options": ["10", "75", "750", "0"],
        "correct": 0,
        "rationale": "You learn one bias term per filter (output map), so 10 biases."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "What does <b>Max Pooling</b> achieve?",
        "options": ["Translation Invariance & Downsampling", "Rotational Invariance", "Color Normalization", "Parameter Reduction"],
        "correct": 0,
        "rationale": "Pooling makes the representation smaller and invariant to small shifts (translations) in the input."
    },
    {
        "topic": "Lec 4: LM",
        "text": "Why do we use <b>Smoothing</b> (e.g. Kneser-Ney)?",
        "options": ["To handle zero-count probabilities", "To reduce vocabulary size", "To speed up training", "To remove stop words"],
        "correct": 0,
        "rationale": "If a word sequence never appeared in training, MLE assigns it 0 probability. Smoothing fixes this infinite perplexity issue."
    },
    {
        "topic": "Lec 5: Attention",
        "text": "In Self-Attention, what connects the current token to other tokens?",
        "options": ["Query-Key Dot Product", "Value-Value Addition", "Fixed Weights", "Convolution"],
        "correct": 0,
        "rationale": "We calculate similarity scores by taking the dot product of the Query vector with all Key vectors."
    },
    {
        "topic": "Lec 6: Pretext",
        "text": "Which is a <b>Generative</b> pretext task?",
        "options": ["Masked Autoencoding (MAE)", "Jigsaw Puzzle", "Rotation Prediction", "Instance Discrimination"],
        "correct": 0,
        "rationale": "MAE tries to reconstruct the pixels (generate data), whereas the others are discriminative classification tasks."
    },
    {
        "topic": "Lec 6: SimCLR",
        "text": "SimCLR uses which loss function?",
        "options": ["InfoNCE", "Mean Squared Error", "KL Divergence", "Hinge Loss"],
        "correct": 0,
        "rationale": "InfoNCE (Noise Contrastive Estimation) maximizes mutual information between views."
    },
    {
        "topic": "Lec 9: Tuning",
        "text": "What is <b>Prompt Tuning</b> (Lester 2021)?",
        "options": ["Optimizing continuous embedding vectors", "Rewriting text prompts manually", "Fine-tuning all model weights", "Using RLHF"],
        "correct": 0,
        "rationale": "Prompt Tuning freezes the model and optimizes a small set of 'soft prompt' vectors at the input layer."
    },
    {
        "topic": "Lec 9: Data",
        "text": "Where does the <b>Reward Signal</b> come from in RLHF?",
        "options": ["A learned Reward Model (RM)", "Direct Human Feedback", "BLEU score", "Ground Truth labels"],
        "correct": 0,
        "rationale": "We train a separate Reward Model on human preferences, then use that model to score generations during RL."
    },
    {
        "topic": "Lec 10: Basics",
        "text": "What tuple defines an <b>MDP</b>?",
        "options": ["(S, A, T, R, gamma)", "(S, A, Loss, Optimizer)", "(Input, Hidden, Output)", "(State, Action, Value)"],
        "correct": 0,
        "rationale": "States, Actions, Transition probability, Reward function, and Discount factor."
    },
    {
        "topic": "Lec 10: Basics",
        "text": "What is <b>Imitation Learning</b>?",
        "options": ["Supervised learning on expert actions", "Learning from rewards", "Unsupervised exploration", "Self-play"],
        "correct": 0,
        "rationale": "Also called Behavioral Cloning; simply trying to predict the action an expert took in a given state."
    },
    {
        "topic": "Lec 11: Algs",
        "text": "Which is an <b>On-Policy</b> algorithm?",
        "options": ["PPO", "DQN", "Q-Learning", "DDPG"],
        "correct": 0,
        "rationale": "PPO requires data collected by the current policy to calculate valid gradient updates."
    },
    {
        "topic": "Lec 11: Algs",
        "text": "What is <b>Model-Based</b> RL?",
        "options": ["Learning a model of dynamics P(s'|s,a)", "Using a pre-trained ResNet", "Model-free learning", "Using a large language model"],
        "correct": 0,
        "rationale": "The agent explicitly learns how the environment works (dynamics) to plan ahead."
    },
    {
        "topic": "Lec 4: LSTM",
        "text": "What does the <b>Input Gate</b> do in an LSTM?",
        "options": ["Decides what new information to store in the cell state", "Decides what output to generate", "Decides what to forget", "Normalizes the input"],
        "correct": 0,
        "rationale": "The Input Gate (sigmoid layer) decides which values we'll update, combined with a tanh layer that creates a vector of new candidate values."
    },
    {
        "topic": "Lec 6: MoCo",
        "text": "What is the benefit of the <b>Queue</b> in MoCo?",
        "options": ["Decouples batch size from the number of negative samples", "Speeds up the momentum update", "Stores the model weights", "Calculates the loss"],
        "correct": 0,
        "rationale": "The queue allows for a large and consistent dictionary of negative samples without needing a massive mini-batch size."
    },
    {
        "topic": "Lec 8: Prompting",
        "text": "What is the key idea behind <b>ReAct</b> prompting?",
        "options": ["Interleaving Reasoning and Acting (tool use)", "Reacting to user emotions", "Repeating the action until success", "Using reinforcement learning"],
        "correct": 0,
        "rationale": "ReAct combines Chain-of-Thought (reasoning) with the ability to take actions (like searching Wikipedia) to solve tasks."
    },
    {
        "topic": "Lec 10: RL",
        "text": "What is the <b>Discount Factor</b> (gamma)?",
        "options": ["Determines how much future rewards are valued vs immediate rewards", "The learning rate", "The probability of taking a random action", "The decay rate of the epsilon greedy strategy"],
        "correct": 0,
        "rationale": "A gamma close to 0 makes the agent myopic (greedy), while a gamma close to 1 makes it strive for long-term high reward."
    },
    {
        "topic": "Lec 11: RL",
        "text": "What is <b>Experience Replay</b> used for?",
        "options": ["Breaking correlations in data for Off-Policy learning", "Replaying the best game to the user", "Simulating future states", "Resetting the environment"],
        "correct": 0,
        "rationale": "By storing and randomly sampling past experiences, we break the temporal correlation of sequential data, stabilizing training for algorithms like DQN."
    },
    {
        "topic": "Lec 5: Transformers",
        "text": "What is the complexity of the <b>Feed Forward</b> layer in terms of hidden dimension d?",
        "options": ["$$ O(d^2) $$", "$$ O(d) $$", "$$ O(d \\log d) $$", "$$ O(1) $$"],
        "correct": 0,
        "rationale": "The FFN involves dense matrix multiplications (usually projecting d to 4d and back), which is quadratic in the model dimension d."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "What is the output size of a 1x1 conv with 64 filters on a 28x28x32 input?",
        "options": ["28x28x64", "28x28x32", "1x1x64", "1x1x32"],
        "correct": 0,
        "rationale": "Spatial dimensions (28x28) are preserved (1x1 kernel). The depth changes to the number of filters (64)."
    },
    {
        "topic": "Lec 9: Tuning",
        "text": "What is <b>Prefix Tuning</b>?",
        "options": ["Prepending learnable vectors to keys/values in every attention layer", "Adding a prefix word to the input string", "Tuning only the first layer", "Using a pre-trained encoder"],
        "correct": 0,
        "rationale": "Prefix Tuning optimizes a sequence of continuous task-specific vectors (prefixes) at every layer of the transformer, not just the input."
    },
    {
        "topic": "Lec 6: SSL",
        "text": "Why is <b>Color Jittering</b> crucial for SimCLR?",
        "options": ["To prevent the model from relying on color histograms to distinguish images", "To make the images look nicer", "To simulate different lighting conditions", "To reduce the image size"],
        "correct": 0,
        "rationale": "Without color jitter, the model can trivially distinguish images based on their color distribution histograms rather than learning semantic features."
    },
    {
        "topic": "Lec 11: RL",
        "text": "What is the <b>Critic</b> in Actor-Critic methods?",
        "options": ["Estimates the Value function V(s) to reduce variance", "Selects the actions", "Critiques the reward function", "Generates the adversarial examples"],
        "correct": 0,
        "rationale": "The Actor learns the policy, while the Critic estimates the value function to provide a lower-variance baseline for the policy update."
    },
    {
        "topic": "Lec 8: Prompting",
        "text": "What is <b>Few-Shot</b> prompting?",
        "options": ["Providing k examples of (input, output) in the context window", "Training on k examples", "Testing on k examples", "Using a model with k parameters"],
        "correct": 0,
        "rationale": "Few-shot prompting leverages the model's in-context learning ability by showing it a few demonstrations of the task at inference time."
    },
    {
        "topic": "Lec 4: RNNs",
        "text": "What is a <b>Bidirectional RNN</b>?",
        "options": ["Processing the sequence both forward and backward and concatenating outputs", "Predicting both the next and previous word", "Having two layers of RNNs", "Sharing weights between encoder and decoder"],
        "correct": 0,
        "rationale": "BiRNNs read the input from left-to-right and right-to-left simultaneously, allowing the current state to have information from both past and future context."
    },
    {
        "topic": "Lec 9: RLHF",
        "text": "What is the <b>Reference Model</b> in RLHF?",
        "options": ["The original SFT model (frozen) used for KL divergence", "The reward model", "The human labeler", "The best performing model"],
        "correct": 0,
        "rationale": "The reference model is a frozen copy of the supervised fine-tuned model. We penalize the RL policy if it diverges too far from this reference."
    },
    {
        "topic": "Lec 3: CNNs",
        "text": "What is the primary motivation for using <b>Dilated Convolutions</b>?",
        "options": ["To exponentially increase the receptive field without losing resolution", "To reduce the number of parameters", "To speed up computation", "To invert the image"],
        "correct": 0,
        "rationale": "Dilation inserts holes in the kernel, expanding its field of view exponentially with depth while keeping the number of parameters constant."
    },
    {
        "topic": "Lec 5: Transformers",
        "text": "What does the <b>Decoder</b> mask in a Transformer do?",
        "options": ["Prevents positions from attending to subsequent positions (future)", "Prevents padding tokens from being processed", "Masks out rare words", "Hides the ground truth label"],
        "correct": 0,
        "rationale": "The decoder mask ensures the autoregressive property: when generating token t, the model cannot 'see' token t+1."
    },
    {
        "topic": "Lec 11: RL",
        "text": "What is <b>Q-Learning</b>?",
        "options": ["An off-policy algorithm that learns the value of state-action pairs", "An on-policy algorithm for policy optimization", "A model-based algorithm", "A genetic algorithm"],
        "correct": 0,
        "rationale": "Q-Learning iteratively updates the Q-values based on the Bellman equation, learning the optimal policy indirectly (Value-based)."
    },
    {
        "topic": "Lec 9: DPO",
        "text": "What distribution does DPO assume for human preferences?",
        "options": ["Bradley-Terry model", "Gaussian distribution", "Poisson distribution", "Uniform distribution"],
        "correct": 0,
        "rationale": "DPO (and RLHF reward modeling) assumes the probability of preferring y_w over y_l follows the Bradley-Terry model based on the difference in implicit rewards."
    }
]